# -*- coding: utf-8 -*-
"""IR_Security_Risks.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aWrqRqLNWt7m-Pe7g_ZO0Mf2KTuEetHc

## What is OWASP?
OWASP stands for the Open Web Application Security Project, an online community that produces articles, methodologies, documentation, tools, and technologies in the field of web application security.

## What is the OWASP Top 10?
OWASP Top 10 is the list of the 10 most common application vulnerabilities. It also shows their risks, impacts, and countermeasures. Updated every three to four years, the latest OWASP vulnerabilities list was released in 2017. Let’s dive into it!

##The Top 10 OWASP vulnerabilities in 2021 are:

*   Injection.
*   Broken authentication.
*   Sensitive data exposure.
*   XML external entities (XXE)
*   Broken access control.
*   Security misconfigurations.
*   Cross site scripting (XSS)
*   Insecure deserialization.
*   Using components with known vulnerabilities
*   Insufficient logging and monitoring

webpage , list of urls, -> values,
credit score(landing club) dataset credibility dataset and genera  finding the type of page

# Web Crawler --
"""

import nltk
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('maxent_ne_chunker')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
stop_words = set(stopwords.words('english'))
nltk.download("words")

import requests #for making HTTP requests in Python
from bs4 import BeautifulSoup # pulling data from HTML or XML files

query = "Injection"
# https://nvd.nist.gov/vuln/search/results?form_type=Basic&results_type=overview&query=Injection&search_type=all&isCpeNameSearch=false

# r = requests.get('https://www.google.com/search?q={}'.format(query))
r = requests.get('https://nvd.nist.gov/vuln/search/results?form_type=Basic&results_type=overview&query=Injection&search_type=all&isCpeNameSearch=false')

soup = BeautifulSoup(r.text, "html.parser") # Using BeautifulSoup we will parse the web page to extraxt the liks from the page
# soup
# print(soup.prettify())

# Extract the useful liks from list of links
links = []
for item1 in soup.find_all('li'):
  for item2 in soup.find_all('a'):
    links.append(item2.get('href'))
# links </li>
print(links)

# Finding the indexed links for Web crawling
final = []
for item in links:
  if item != None :
    if len(item) >= 120:
      # stri = item[0:11]
      # if stri == "/vuln/search/":
      final.append(item)
final       # final list of liks    
print(final)
len(final)
print(final[1])

url_1 = 'https://nvd.nist.gov' + final[1]
print(url_1)

#webpage1 = requests.get('https://nvd.nist.gov' + final[0]) #final[0] refers to the first web page link
query = "Injection"
url_nvd = "https://nvd.nist.gov/vuln/search/results?form_type=Basic&results_type=overview&query=Injection&search_type=all&isCpeNameSearch=false"
url_wiki = 'https://en.wikipedia.org/wiki/SQL_injection'
def crawler(url):
    webpage = requests.get(url)
    webpagetext = BeautifulSoup(webpage.text, "html.parser")
    all_p = webpagetext.find_all('p')
    text = ""
    for item in all_p:
        text = text + item.get_text()
    return text
text_nvd = crawler(url_nvd)    
text_wiki = crawler(url_wiki)    
print(text_nvd,text_wiki)

"""## Text (sentence) from wiki for "Injection"

## Text extraction from the intractive fields
"""

# import json
# import urllib

# # url for the request,  url_nvd
# response = urllib.request.urlopen(url_nvd)

# # load as json
# jresponse = json.load(response)

# # write to file as pretty print
# with open('asdaresp.json', 'w') as outfile:
#     json.dump(jresponse, outfile, sort_keys=True, indent=4)

# !pip install selenium

# !pip install chromedriver

from selenium import webdriver
from selenium.webdriver.chrome.options import Options

options = Options()
options.headless = True
options.add_argument("--window-size=1920,1200")
driver = webdriver.Chrome(options=options, executable_path=r'/home/ganesh/Downloads/RL_project/chromedriver')
driver.get(url_nvd)
driver.save_screenshot('hn_homepage.png')
driver.quit()

import gensim
# Operating System
import os
# Regular Expression
import re
# nltk packages
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.stem.porter import *
from nltk.stem.snowball import SnowballStemmer
# Basic Packages
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
# PCA Package
from sklearn.decomposition import PCA
stemmer = SnowballStemmer("english")

stopWords = pd.read_csv('stopwords.txt').values

class MySentences(object):
    def __init__(self, fnamelist):
        self.fnamelist = fnamelist
        # Creating a set of vocabulary
        self.vocabulary = set([])

    def __iter__(self):
        for fname in self.fnamelist:
            for line in open(fname, encoding='latin1'):
                # Find all the words that has letters from 2 - 15. If the words are longer than that ignore.
                words = re.findall(r'(\b[A-Za-z][a-z]{2,15}\b)', line)
                # Stemming a word.
                words = [ stemmer.stem(word.lower()) for word in words if not word.lower() in stopWords]
                for word in words:
                    self.vocabulary.add(word)
                yield words
sentences_nvd = MySentences(text_nvd) # a memory-friendly iterator 
sentences_wiki = MySentences(text_wiki) # a memory-friendly iterator

from nltk.tokenize import word_tokenize, sent_tokenize
tokenized_nvd = sent_tokenize(text_nvd)
tokenized_wiki = sent_tokenize(text_wiki)

text_nvd = crawler(url_nvd)    # input nvd url for the keyword e.g., Injection, return the text from the nvd website
text_wiki = crawler(url_wiki)  # input wiki url for the keyword e.g., Injection, return the text from the wiki website
sentences_nvd = MySentences(text_nvd) # a memory-friendly iterator
tokenized_nvd = sent_tokenize(text_nvd)

"""Finally, we create an inverted index. We maintain a secondary list called is present store the words that are already present in the inverted index. The variable invind stores the inverted index."""



"""### Let’s create some sentences, initialize our model, and encode the sentences:

Take a sentence, convert it into a vector.
Take many other sentences, and convert them into vectors.
Find sentences that have the smallest distance (Euclidean) or smallest angle (cosine similarity) between them — more on that here.
We now have a measure of semantic similarity between sentences — easy!
"""

# Write a few sentences to encode (sentences 0 and 2 are both similar):
# sentences = [
#     "Three years later, the coffin was still full of Jello.",
#     "The fish dreamed of escaping the fishbowl and into the toilet where he saw his friend go.",
#     "The person box was packed with jelly many dozens of months later.",
#     "He found a leprechaun in his walnut shell."
# ]

# sentences = sentences.append(text_wiki)
# print(sentences[0])

# !pip install sentence-transformers
# !pip install huggingface-hub==0.0.12

"""Encode the sentences:

###  Now what we do is take those embeddings and find the cosine similarity between each. So for sentence 0: 
We can find the most similar sentence using:
"""

# Initialize our model:
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

print(type(tokenized_nvd))
sentences = [tokenized_nvd, tokenized_wiki]
print(type(sentences))
model = SentenceTransformer('bert-base-nli-mean-tokens')


sentence_embeddings = model.encode(sentences)
# sentence_embeddings.shape ##


similarity = cosine_similarity( 
    [sentence_embeddings[0]],
    sentence_embeddings[1:])
similarity

"""Great, we now have four sentence embeddings — each containing 768 values.

"""

print(similarity[0][0])

"""| Index | Sentence | Similarity wiki text |
| --- | --- | --- |
| 1 | Test from NVD for Injuctions. | 0.7407259 |
| 2 |  Text from Wiki page for Injections.     | 0.6837034 | 
| 3 |   Text from NVD for Sensitive data exposure (sde)   | 0.5548 |

## Simillar vulnerabilities for (Injection)
"""



"""### Additionally, before the mean pooling operation, we need to create last_hidden_state, which we do like so:

"""

from transformers import AutoTokenizer, AutoModel
import torch

tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')
model = AutoModel.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')

sentences = [
    "Three years later, the coffin was still full of Jello.",
    "The fish dreamed of escaping the fishbowl and into the toilet where he saw his friend go.",
    "The person box was packed with jelly many dozens of months later.",
    "He found a leprechaun in his walnut shell."
]

# initialize dictionary to store tokenized sentences
tokens = {'input_ids': [], 'attention_mask': []}

for sentence in sentences:
    # encode each sentence and append to dictionary
    new_tokens = tokenizer.encode_plus(sentence, max_length=128,
                                       truncation=True, padding='max_length',
                                       return_tensors='pt')
    tokens['input_ids'].append(new_tokens['input_ids'][0])
    tokens['attention_mask'].append(new_tokens['attention_mask'][0])

# reformat list of tensors into single tensor
tokens['input_ids'] = torch.stack(tokens['input_ids'])
tokens['attention_mask'] = torch.stack(tokens['attention_mask'])

# We process these tokens through our model:

outputs = model(**tokens)
outputs.keys()

# The dense vector representations of our text are contained within the outputs 
# 'last_hidden_state' tensor, which we access like so:
embeddings = outputs.last_hidden_state
embeddings

# 
embeddings.shape

"""After we have produced our dense vectors embeddings, we need to perform a mean pooling operation to create a single vector encoding (the sentence embedding).


*  To do this mean pooling operation, we will need to multiply each value in our embeddings tensor by its respective attention_mask value — so that we ignore non-real tokens.

"""

# To perform this operation, we first resize our attention_mask tensor:
attention_mask = tokens['attention_mask']
attention_mask.shape

mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()
mask.shape

mask

# Each vector above represents a single token attention mask  
#  -each token now has a vector of size 768 representing it's attention_mask status.
# Then we multiply the two tensors to apply the attention mask:


masked_embeddings = embeddings * mask
masked_embeddings.shape

masked_embeddings
# Then we sum the remained of the embeddings along axis 1:
summed = torch.sum(masked_embeddings, 1)
summed.shape

#Then sum the number of values that must be given attention in each position of the tensor:
summed_mask = torch.clamp(mask.sum(1), min=1e-9)
summed_mask.shape
summed_mask

"""Finally, we calculate the mean as the sum of the embedding activations summed divided by the number of values that should be given attention in each position summed_mask:"""

mean_pooled = summed / summed_mask
mean_pooled

"""Once we have our dense vectors, we can calculate the cosine similarity between each — which is the same logic we used before:"""

from sklearn.metrics.pairwise import cosine_similarity

# Let's calculate cosine similarity for sentence 0:

# convert from PyTorch tensor to numpy array
mean_pooled = mean_pooled.detach().numpy()

# calculate
cosine_similarity(
    [mean_pooled[0]],
    mean_pooled[1:]
)

"""These similarities translate to:"""

# from scipy import spatial
# from sent2vec.vectorizer import Vectorizer

# sentences = [
#     "This is an awesome book to learn NLP.",
#     "DistilBERT is an amazing NLP model.",
#     "We can interchangeably use embedding, encoding, or vectorizing.",
# ]

# vectorizer = Vectorizer()
# vectorizer.bert(sentences)
# vectors_bert = vectorizer.vectors

# dist_1 = spatial.distance.cosine(vectors_bert[0], vectors_bert[1])
# dist_2 = spatial.distance.cosine(vectors_bert[0], vectors_bert[2])
# print('dist_1: {0}, dist_2: {1}'.format(dist_1, dist_2))
# # dist_1: 0.043, dist_2: 0.192

# see data, choose the th, range, stats, Simillarty score
# web crawling,Rl page to page, 
# How dose the model to get rewarded, papers, reviews

"""## For every document, we first tokenize the document into individual words.

## CVSS score
"""

# !pip install cvss

from cvss import CVSS2, CVSS3


vector = 'AV:L/AC:L/Au:M/C:N/I:P/A:C/E:U/RL:W/RC:ND/CDP:L/TD:H/CR:ND/IR:ND/AR:M'
c = CVSS2(vector)
print(vector)
print(c.clean_vector())
print(c.scores())

print()

vector = 'CVSS:3.0/S:C/C:H/I:H/A:N/AV:P/AC:H/PR:H/UI:R/E:H/RL:O/RC:R/CR:H/IR:X/AR:X/MAC:H/MPR:X/MUI:X/MC:L/MA:X'
c = CVSS3(vector)
print(vector)
print(c.clean_vector())
print(c.scores())
print(c.severities())



"""# Deep-Deep: Adaptive Crawler (using RL)
Deep-Deep is a Scrapy-based crawler which uses Reinforcement Learning methods to learn which links to follow.

It is called Deep-Deep, but it doesn't use Deep Learning, and it is not only for Deep web. Weird

You can use deep-deep to just run adaptive crawls, updating link model and collecting crawled data at the same time. But in some cases it is more efficient to first train a link model with deep-deep, and then use this model in another crawler. Deep-deep uses a lot of memory to store page and link features, and more CPU to update the link model. So if the link model is general enough to freeze it, you can run a more efficient crawl. Or you might want to just use deep-deep link model in an existing project.
[link](https://github.com/TeamHG-Memex/deep-deep)

## url-summary
Show summary of a large number of URLs in a Jupyter Notebook: analyze domains, paths, query keys and values. This is useful if you want to have a quick glance at URLs obtained by crawling.

[link](https://github.com/TeamHG-Memex/url-summary)

Build a crawler, visit page(input value) and decide- related and compare to wiki_text, (reward) 

 NVD , input/ output, Crawling (decision) random web crawling, sq pro,https://www.virustotal.com/gui/

# RL model

## **ϵ (epsilon)-greedy algorithm**
One very famous approach to solving reinforcement learning problems is the ϵ (epsilon)-greedy algorithm, such that, with a probability ϵ, you will choose an action a at random (exploration), and the rest of the time (probability 1−ϵ) you will select the best lever based on what you currently know from past plays (exploitation). So most of the time you play greedy, but sometimes you take some risks and choose a random lever and see what happens.

First, import the necessary libraries and modules required to implement the algorithm.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import random
import matplotlib.pyplot as plt
# %matplotlib inline
np.random.seed(15)

"""We'll be solving the 10-pages visited one time, hence n = 10. is a numpy array of length n filled with random floats that can be understood as probabilities of action of that page."""

n = 10 # no of pages 
arms = np.random.rand(n) # initial random values for the 10 websites
print(arms)
eps = 0.1 #probability of exploration action, goto next page

# defining the reward using similarty of the visited page and wiki page
# print(similarity[0][0])
def reward(prob):
    reward = 0
    for i in range(10):
        if similarity[0][0] < prob:
            reward += 1
    return reward

#initialize memory array; has 1 row defaulted to random action index
av = np.array([np.random.randint(0,(n+1)), 0]).reshape(1,2) #av = action-value
print(av)
av

"""The reward functions work as such - for each page, you run a loop of 10 iterations, and generate a random float every time. If this random number is less than the probability of that arm, you'll add a 1 to the reward. After all iterations, you'll have a value between 0 to 10."""

#greedy method to select best page based on memory array
def bestPage(a):
    bestPage = 0 #default to 0
    bestMean = 0
    for u in a:
        avg = np.mean(a[np.where(a[:,0] == u[0])][:, 1]) #calculate mean reward for each action
        if bestMean < avg:
            bestMean = avg
            bestPage = u[0]
    return bestPage

plt.xlabel("Number of times visit page")
plt.ylabel("Average Reward")
for i in range(500):
    if random.random() > eps: #greedy exploitation action
        choice = bestPage(av)
        thisAV = np.array([[choice, reward(arms[choice])]])
        av = np.concatenate((av, thisAV), axis=0)
    else: #exploration action
        choice = np.where(arms == np.random.choice(arms))[0][0]
        thisAV = np.array([[choice, reward(arms[choice])]]) #choice, reward
        av = np.concatenate((av, thisAV), axis=0) #add to our action-value memory array
    #calculate the mean reward
    runningMean = np.mean(av[:,1])
    plt.scatter(i, runningMean)

# random.random()
print(av.size)

"""# importing lib"""

import gym
import itertools
import matplotlib
import matplotlib.style
import numpy as np
import pandas as pd
import sys
  
  
from collections import defaultdict
# from windy_gridworld import WindyGridworldEnv
# import plotting
  
matplotlib.style.use('ggplot')

# Create gym environment.
# env = WindyGridworldEnv()

def createEpsilonGreedyPolicy(Q, epsilon, num_actions):
    """
    Creates an epsilon-greedy policy based
    on a given Q-function and epsilon.
       
    Returns a function that takes the state
    as an input and returns the probabilities
    for each action in the form of a numpy array 
    of length of the action space(set of possible actions).
    """
    def policyFunction(state):
   
        Action_probabilities = np.ones(num_actions,
                dtype = float) * epsilon / num_actions
                  
        best_action = np.argmax(Q[state])
        Action_probabilities[best_action] += (1.0 - epsilon)
        return Action_probabilities
   
    return policyFunction

def qLearning(env, num_episodes, discount_factor = 1.0,
                            alpha = 0.6, epsilon = 0.1):
    """
    Q-Learning algorithm: Off-policy TD control.
    Finds the optimal greedy policy while improving
    following an epsilon-greedy policy"""
       
    # Action value function
    # A nested dictionary that maps
    # state -> (action -> action-value).
    Q = defaultdict(lambda: np.zeros(env.action_space.n))
   
    # Keeps track of useful statistics
    stats = plotting.EpisodeStats(
        episode_lengths = np.zeros(num_episodes),
        episode_rewards = np.zeros(num_episodes))    
       
    # Create an epsilon greedy policy function
    # appropriately for environment action space
    policy = createEpsilonGreedyPolicy(Q, epsilon, env.action_space.n)
       
    # For every episode
    for ith_episode in range(num_episodes):
           
        # Reset the environment and pick the first action
        state = env.reset()
           
        for t in itertools.count():
               
            # get probabilities of all actions from current state
            action_probabilities = policy(state)
   
            # choose action according to 
            # the probability distribution
            action = np.random.choice(np.arange(
                      len(action_probabilities)),
                       p = action_probabilities)
   
            # take action and get reward, transit to next state
            next_state, reward, done, _ = env.step(action)
   
            # Update statistics
            stats.episode_rewards[ith_episode] += reward
            stats.episode_lengths[ith_episode] = t
               
            # TD Update
            best_next_action = np.argmax(Q[next_state])    
            td_target = reward + discount_factor * Q[next_state][best_next_action]
            td_delta = td_target - Q[state][action]
            Q[state][action] += alpha * td_delta
   
            # done is True if episode terminated   
            if done:
                break
                   
            state = next_state
       
    return Q, stats

Q, stats = qLearning(env, 1000)

plotting.plot_episode_stats(stats)











lemmatizer = WordNetLemmatizer() #we instantiate the lemmatizer using the WordNetLemmatizer library
stop = set(stopwords.words('english'))
for y,x in enumerate(doc):
  tokenstop = [item for item in word_tokenize(x) if (item.isalpha() or item.isdigit()) and not item in stop]
  #creating a second list called lemm that stores the lemmatized words
  lemm = [lemmatizer.lemmatize(item) for item in tokenstop]
  for word in lemm:
    if word not in invind.keys():
      ispresent[word] = [y]
      invind[word] = [[y,1]]
    elif y not in ispresent[word]:
      invind[word].append([y,1])
      ispresent[word].append(y)
    else:
      for i in range(len(invind[word])):
        if invind[word][i][0]==y:
            invind[word][i][1]+=1

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import pandas as pd
stopwords = pd.read_csv('stopwords.txt').values
# stop = set(stopwords.('english')) #stopwords list
# doc is a list of many document texts 
for y,x in enumerate(doc):
  tokenstop = [item for item in word_tokenize(x) if (item.isalpha() or item.isdigit()) and not item in stop]


lemmatizer = WordNetLemmatizer() #we instantiate the lemmatizer using the WordNetLemmatizer library
stop = set(stopwords.words('english'))
for y,x in enumerate(doc):
  tokenstop = [item for item in word_tokenize(x) if (item.isalpha() or item.isdigit()) and not item in stop]
  #creating a second list called lemm that stores the lemmatized words
  lemm = [lemmatizer.lemmatize(item) for item in tokenstop]

# input, output, 4-5 websites (e.g., NVD) 
# Docomentation:
#  Process dig., (steps, algo. methods) 
#  writeup, approach dig. , process diag.  
# A2C
# D-Qn
# We want , domain,



"""## A2C"""

import numpy as np
import torch
import gym
from torch import nn
import matplotlib.pyplot as plt



# helper function to convert numpy arrays to tensors
def t(x): return torch.from_numpy(x).float()

# Actor module, categorical actions only
class Actor(nn.Module):
    def __init__(self, state_dim, n_actions):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.Tanh(),
            nn.Linear(64, 32),
            nn.Tanh(),
            nn.Linear(32, n_actions),
            nn.Softmax()
        )
    
    def forward(self, X):
        return self.model(X)

# Critic module
class Critic(nn.Module):
    def __init__(self, state_dim):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )
    
    def forward(self, X):
        return self.model(X)

env = gym.make("CartPole-v1")

# config
state_dim = env.observation_space.shape[0] # 4
n_actions = env.action_space.n # 2
actor = Actor(state_dim, n_actions)
critic = Critic(state_dim)
adam_actor = torch.optim.Adam(actor.parameters(), lr=1e-3)
adam_critic = torch.optim.Adam(critic.parameters(), lr=1e-3)
gamma = 0.99

print(n_actions)

episode_rewards = []

for i in range(500):
    done = False
    total_reward = 0
    state = env.reset()


    while not done:
        probs = actor(t(state))
        dist = torch.distributions.Categorical(probs=probs)
        action = dist.sample()
        
        next_state, reward, done, info = env.step(action.detach().data.numpy())
        advantage = reward + (1-done)*gamma*critic(t(next_state)) - critic(t(state))
        
        total_reward += reward
        state = next_state

        critic_loss = advantage.pow(2).mean()
        adam_critic.zero_grad()
        critic_loss.backward()
        adam_critic.step()

        actor_loss = -dist.log_prob(action)*advantage.detach()
        adam_actor.zero_grad()
        actor_loss.backward()
        adam_actor.step()
            
    episode_rewards.append(total_reward)

plt.scatter(np.arange(len(episode_rewards)), episode_rewards, s=2)
plt.title("Total reward per episode (online)")
plt.ylabel("reward")
plt.xlabel("episode")
plt.show()

